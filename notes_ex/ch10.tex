\chapter{Generalized Linear Models}

This is a re-derivation of the result that the normal distribution with variance
$\sigma^2$ has the largest entropy amongst all distributions defined on
$[-\infty, +\infty]$ with variance $\sigma^2$. Let $p(x)$ be the pdf of the
normal distribution with mean $\mu$ and variance $\sigma^2$. Let $q(x)$ be a
pdf with the same variance. Since the entropy of a distribution does not depend
on its mean, we may assume that the mean of $q(x)$ is $\mu$.

The entropy $H(p)$ of the normal distribution is:
\begin{align*}
    H(p) & = - \int_{- \infty}^{+ \infty} p(x) \log p(x) \der x \\
        & = - \int_{- \infty}^{+ \infty} p(x) \log ((2 \pi \sigma^2)^{-1/2}) \der x
            + \int_{- \infty}^{+ \infty} p(x) \frac{1}{2} \left ( \frac{x - \mu}{\sigma} \right )^2 \der x \\
        & = \frac{1}{2} \log (2 \pi \sigma^2)
            + \frac{1}{2 \sigma} \int_{- \infty}^{+ \infty} \frac{1}{\sqrt{2 \pi}} \left ( \frac{x - \mu}{\sigma} \right )^2
            \exp \left \{- \left ( \frac{x - \mu}{\sigma} \right )^2 \right \} \der x \\
        & = \frac{1}{2} \log (2 \pi \sigma^2) +
            \frac{1}{2 \sigma} \int_{- \infty}^{+ \infty}
            \frac{\sigma}{\sqrt{2 \pi}} z^2 \exp \left \{- \frac{z^2}{2} \right \} \der z \\
        & = \frac{1}{2} \log (2 \pi \sigma^2) + \frac{1}{2} \E [Z^2] \\
        & = \frac{1}{2} \log (2 \pi \sigma^2) + \frac{1}{2} (\Var(Z) + (\E [Z])^2) \\
        & = \frac{1}{2} \log (2 \pi e \sigma^2).
\end{align*}

Now that we have the entropy of the normal, we use the KL-divergence metric
to measure the distance of $q(x)$ from $p(x)$.
\begin{align*}
    \kl (q, p) = \int_{- \infty}^{+ \infty} q(x) \log \frac{q(x)}{p(x)} \der x = - H(q) + H(q, p).
\end{align*}
At this point, we use the fact that $\kl (q, p) \geq 0$ for all distributions $q$ and $p$.
This gives us: $H(q, p) \geq H(q)$. We do not know what $H(q)$ is but the expression
for $H(q, p)$ can be evaluated quite easily.
\begin{align*}
    H(q, p) & = - \int_{- \infty}^{+ \infty} q(x) \log p(x) \der x \\
            & =  - \int_{- \infty}^{+ \infty} q(x)
                \left [ \log \frac{1}{\sqrt{2 \pi \sigma^2}}
                - \frac{1}{2} \left ( \frac{x - \mu}{\sigma} \right )^2 \right ] \der x \\
            & = - \log \frac{1}{\sqrt{2 \pi \sigma^2}}  \int_{- \infty}^{+ \infty} q(x) \der x +
                \frac{1}{2 \sigma^2} \int_{- \infty}^{+ \infty} q(x) ( x - \mu )^2 \der x\\
            & = \frac{1}{2} \log (2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \E_{q} [(x - \mu)^2] \\
            & = \frac{1}{2} \log (2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \sigma^2 \\
            & = \frac{1}{2} \log (2 \pi \sigma^2) + \frac{1}{2} \\
            & = \frac{1}{2} \log (2 \pi e \sigma^2) \\
            & = H(p).
\end{align*}
So $H(q, p) = H(p)$ and combining this with $H(q, p) \geq H(q)$, we
obtain $H(p) \geq H(q)$.

\section{Binomial Distributions and Maximum Entropy}

Just as the normal distribution is the one with maximum entropy amongst all distributions that have
a constant variance, the binomial is a distribution with the maximum entropy amongst
all distributions defined on an experiment with just two outcomes and with a constant mean.

To make this precise, consider binary sequences $S = \{ s_i \}_{i = 0}^{2^n - 1}$
of length~$n$ sampled from a binomial distribution $\Binom(n, p)$. Let $x_i$ denote
the number of $1$s in~$s_i$; let $p_i = \Prone{s_i} = p^{x_i} (1 - p)^{n - x_i}$ and define
$\vec{p} = [p_0, \ldots, p_{2^n - 1}]$. Let $\vec{q} = [q_0, \ldots, q_{2^n - 1}]$ be some
distribution over $S$. We wish to show that $H(p) \geq H(q)$. As in the case of the
normal distribution, we start with the KL-divergence metric. We have
\[
    \kl (\vec{q}, \vec{p}) = \sum_{i = 0}^{2^n - 1} q_i \log \frac{q_i}{p_i}
        = - H(\vec{q}) + H(\vec{q}, \vec{p}) \geq 0,
\]
which yields that $H(\vec{q}, \vec{p}) \geq H(\vec{q})$.

Now consider the expression $H(\vec{q}, \vec{p})$. We may write:
\begin{align*}
    H(\vec{q}, \vec{p}) & = - \sum_{i = 0}^{2^n - 1} q_i \log p_i \\
        & = - \sum_{i = 0}^{2^n - 1} q_i \log \left ( p^{x_i} (1 - p)^{n - x_i} \right ) \\
        & = - \sum_{i = 0}^{2^n - 1} q_i \left [ x_i \log \frac{p}{1 - p} + n \log (1 - p) \right ] \\
        & = - \left ( \log \frac{p}{1 - p} \right ) \sum_{i = 0}^{2^n - 1} q_i x_i - n \log (1 - p) \sum_{i = 0}^{2^n - 1} q_i \\
        & = - \left ( \log \frac{p}{1 - p} \right ) \bar{q} - n \log (1 - p).
\end{align*}
Here $\bar{q}$ is the expected value of the distribution $\vec{q}$. If we assume that
$\bar{q} = \sum_{i = 0}^{2^n - 1} p_i x_i$, then we can retrace
the steps in the last derivation and show that $H(\vec{q}, \vec{p}) = H(\vec{p})$. Combining this with the
inequality $H(\vec{q}, \vec{p}) \geq H(\vec{q})$, we obtain that $H(\vec{p}) \geq H(\vec{q})$. That is, any
distribution $\vec{q}$ on binary sequences of length~$n$ with the same expected value $\sum_{i = 0}^{2^n - 1} p_i x_i$
as $\vec{p}$, has entropy at most that of $\vec{p}$.

As a matter of fact,
\begin{align*}
    \sum_{i = 0}^{2^n - 1} p_i x_i
        & = \sum_{k = 0}^{n} {n \choose k} k p^k (1 - p)^{n - k} \\
        & = \sum_{k = 0}^{n} \frac{n!}{k! (n - k)!} k p^k (1 - p)^{n - k} \\
        & = \sum_{k = 1}^{n} \frac{n!}{(k - 1)! (n - k)!} p^k (1 - p)^{n - k} \\
        & = np \sum_{k = 1}^{n} \frac{(n - 1)!}{(k - 1)! (n - k)!} p^{k - 1} (1 - p)^{n - k} \\
        & =  np \sum_{j = 0}^{n - 1} \frac{(n - 1)!}{j! (n - 1 - j)!} p^{j} (1 - p)^{n - 1 - j} \\
        & = np \left ( p + (1 - p) \right )^n \\
        & = np,
\end{align*}
which matches with intuitive result that if the probability of a $1$ is $p$, then the expected
number of $1$s in $n$ trials is $np$.
