\chapter{Generalized Linear Models}

This is a re-derivation of the result that the normal distribution with variance
$\sigma^2$ has the largest entropy amongst all distributions defined on
$[-\infty, +\infty]$ with variance $\sigma^2$. Let $p(x)$ be the pdf of the
normal distribution with mean $\mu$ and variance $\sigma^2$. Let $q(x)$ be a
pdf with the same variance. Since the entropy of a distribution does not depend
on its mean, we may assume that the mean of $q(x)$ is $\mu$.

The entropy $H(p)$ of the normal distribution is:
\begin{align*}
    H(p) & = - \int_{- \infty}^{+ \infty} p(x) \log p(x) \der x \\
        & = - \int_{- \infty}^{+ \infty} p(x) \log ((2 \pi \sigma^2)^{-1/2}) \der x
            + \int_{- \infty}^{+ \infty} p(x) \frac{1}{2} \left ( \frac{x - \mu}{\sigma} \right )^2 \der x \\
        & = \frac{1}{2} \log (2 \pi \sigma^2)
            + \frac{1}{2 \sigma} \int_{- \infty}^{+ \infty} \frac{1}{\sqrt{2 \pi}} \left ( \frac{x - \mu}{\sigma} \right )^2
            \exp \left \{- \left ( \frac{x - \mu}{\sigma} \right )^2 \right \} \der x \\
        & = \frac{1}{2} \log (2 \pi \sigma^2) +
            \frac{1}{2 \sigma} \int_{- \infty}^{+ \infty}
            \frac{\sigma}{\sqrt{2 \pi}} z^2 \exp \left \{- \frac{z^2}{2} \right \} \der z \\
        & = \frac{1}{2} \log (2 \pi \sigma^2) + \frac{1}{2} \E [Z^2] \\
        & = \frac{1}{2} \log (2 \pi \sigma^2) + \frac{1}{2} (\Var(Z) + (\E [Z])^2) \\
        & = \frac{1}{2} \log (2 \pi e \sigma^2).
\end{align*}

Now that we have the entropy of the normal, we use the KL-divergence metric
to measure the distance of $q(x)$ from $p(x)$.
\begin{align*}
    \kl (q, p) = \int_{- \infty}^{+ \infty} q(x) \log \frac{q(x)}{p(x)} \der x = - H(q) + H(q, p).
\end{align*}
At this point, we use the fact that $\kl (q, p) \geq 0$ for all distributions $q$ and $p$.
This gives us: $H(q, p) \geq H(q)$. We do not know what $H(q)$ is but the expression
for $H(q, p)$ can be evaluated quite easily.
\begin{align*}
    H(q, p) & = - \int_{- \infty}^{+ \infty} q(x) \log p(x) \der x \\
            & =  - \int_{- \infty}^{+ \infty} q(x)
                \left [ \log \frac{1}{\sqrt{2 \pi \sigma^2}}
                - \frac{1}{2} \left ( \frac{x - \mu}{\sigma} \right )^2 \right ] \der x \\
            & = - \log \frac{1}{\sqrt{2 \pi \sigma^2}}  \int_{- \infty}^{+ \infty} q(x) \der x +
                \frac{1}{2 \sigma^2} \int_{- \infty}^{+ \infty} q(x) ( x - \mu )^2 \der x\\
            & = \frac{1}{2} \log (2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \E_{q} [(x - \mu)^2] \\
            & = \frac{1}{2} \log (2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \sigma^2 \\
            & = \frac{1}{2} \log (2 \pi \sigma^2) + \frac{1}{2} \\
            & = \frac{1}{2} \log (2 \pi e \sigma^2) \\
            & = H(p).
\end{align*}
So $H(q, p) = H(p)$ and combining this with $H(q, p) \geq H(q)$, we
obtain $H(p) \geq H(q)$.

\section{Binomial Distributions and Maximum Entropy}

Just as the normal distribution is the one with maximum entropy amongst all distributions that have
a constant variance, the binomial is a distribution with the maximum entropy amongst
all distributions defined on an experiment with just two outcomes and with a constant mean.

To make this precise, consider binary sequences $S = \{ s_i \}_{i = 0}^{2^n - 1}$
of length~$n$ sampled from a binomial distribution $\Binom(n, p)$. Let $x_i$ denote
the number of $1$s in~$s_i$; let $p_i = \Prone{s_i} = p^{x_i} (1 - p)^{n - x_i}$ and define
$\vect{p} = [p_0, \ldots, p_{2^n - 1}]$. Let $\vect{q} = [q_0, \ldots, q_{2^n - 1}]$ be some
distribution over $S$. We wish to show that $H(p) \geq H(q)$. As in the case of the
normal distribution, we start with the KL-divergence metric. We have
\[
    \kl (\vect{q}, \vect{p}) = \sum_{i = 0}^{2^n - 1} q_i \log \frac{q_i}{p_i}
        = - H(\vect{q}) + H(\vect{q}, \vect{p}) \geq 0,
\]
which yields that $H(\vect{q}, \vect{p}) \geq H(\vect{q})$.

Now consider the expression $H(\vect{q}, \vect{p})$. We may write:
\begin{align*}
    H(\vect{q}, \vect{p}) & = - \sum_{i = 0}^{2^n - 1} q_i \log p_i \\
        & = - \sum_{i = 0}^{2^n - 1} q_i \log \left ( p^{x_i} (1 - p)^{n - x_i} \right ) \\
        & = - \sum_{i = 0}^{2^n - 1} q_i \left [ x_i \log \frac{p}{1 - p} + n \log (1 - p) \right ] \\
        & = - \left ( \log \frac{p}{1 - p} \right ) \sum_{i = 0}^{2^n - 1} q_i x_i - n \log (1 - p) \sum_{i = 0}^{2^n - 1} q_i \\
        & = - \left ( \log \frac{p}{1 - p} \right ) \bar{q} - n \log (1 - p).
\end{align*}
Here $\bar{q}$ is the expected value of the distribution $\vect{q}$. If we assume that
$\bar{q} = \sum_{i = 0}^{2^n - 1} p_i x_i$, then we can retrace
the steps in the last derivation and show that $H(\vect{q}, \vect{p}) = H(\vect{p})$. Combining this with the
inequality $H(\vect{q}, \vect{p}) \geq H(\vect{q})$, we obtain that $H(\vect{p}) \geq H(\vect{q})$. That is, any
distribution $\vect{q}$ on binary sequences of length~$n$ with the same expected value $\sum_{i = 0}^{2^n - 1} p_i x_i$
as $\vect{p}$, has entropy at most that of $\vect{p}$.

As a matter of fact,
\begin{align*}
    \sum_{i = 0}^{2^n - 1} p_i x_i
        & = \sum_{k = 0}^{n} {n \choose k} k p^k (1 - p)^{n - k} \\
        & = \sum_{k = 0}^{n} \frac{n!}{k! (n - k)!} k p^k (1 - p)^{n - k} \\
        & = \sum_{k = 1}^{n} \frac{n!}{(k - 1)! (n - k)!} p^k (1 - p)^{n - k} \\
        & = np \sum_{k = 1}^{n} \frac{(n - 1)!}{(k - 1)! (n - k)!} p^{k - 1} (1 - p)^{n - k} \\
        & =  np \sum_{j = 0}^{n - 1} \frac{(n - 1)!}{j! (n - 1 - j)!} p^{j} (1 - p)^{n - 1 - j} \\
        & = np \left ( p + (1 - p) \right )^n \\
        & = np,
\end{align*}
which matches with intuitive result that if the probability of a $1$ is $p$, then the expected
number of $1$s in $n$ trials is $np$.

\section{The Poisson and Related Families of Distributions}

The Poisson distribution is closely connected to distributions such as the binomial, exponential and the discrete Gamma
(known as the Erlang distribution). To understand this connection, we begin with a description of Poisson process.
A Poisson process is an arrival process that is described by a function $P(k, \tau)$ that gives the probability
of $k$ arrivals in a time interval of length $\tau$ and which satisfies the following conditions:
\begin{enumerate}
    \item $P(k, \tau)$ is the same for all intervals of length $\tau$;
    \item the number of arrivals during a particular time interval is independent of the history of arrivals outside
        this time interval;
    \item there exists a constant $\lambda$ such that:
        \begin{itemize}
            \item $P(0, \tau) = 1 - \lambda \tau + o(\tau)$
            \item $P(1, \tau) = \lambda \tau + o_1(\tau)$
            \item $P(k, \tau) = o_k(\tau)$ for $k \in \{2, 3, \ldots\}$
        \end{itemize}
\end{enumerate}
The last condition states that in the regime of small time intervals, the probability of a single arrival in a time interval
of length $\tau$ is proportional to the length of the time interval. The functions $o(\tau), o_k(\tau)$ satisfy the conditions:
\[
    \lim_{\tau \to 0} \frac{o(\tau)}{\tau} = 0, \quad \lim_{\tau \to 0} \frac{o_k(\tau)}{\tau} \text{ for } k \in \{1, 2, \ldots\}.
\]
The functions $o(\tau)$ and $o_k(\tau)$ can be thought of as the higher-order terms in a Taylor series expansion of function of $\tau$.

\subsection{Number of arrivals}
In order to compute a closed-form expression for $P(k, \tau)$, imagine dividing the time interval $\tau$ in $n$ equal disjoint
pieces intervals, each of length $\delta$. If $n$ is large enough, then the probability of a single arrival in a time interval of length $\delta$
is approximately $\lambda \delta$ and that of two or more arrivals is $0$. Since the arrivals in each time window of length
$\delta$ is independent of the arrivals in the other intervals, we may approximate this using a Bernoulli distribution with
success probability $p = \lambda \delta$ and $n = \tau / \delta$ trials. The point to note is that $np = \lambda \tau$
is a constant. Thus the probability of $k$ successes is:
\begin{align*}
    P(k, \tau) & = {n \choose k} p^{k} (1 - p)^{n - k} \\
               & = \frac{n!}{(n - k)! k!} \left ( \frac{\lambda \tau}{n}\right )^k \left ( 1 - \frac{\lambda \tau}{n} \right )^{n - k} \\
               & = \frac{n (n - 1) \cdots (n - k + 1)}{n^k} \cdot \frac{1}{k!} \cdot (\lambda \tau)^{k}
                    \cdot \left ( 1 - \frac{\lambda \tau}{n} \right )^{-k} \cdot \left ( 1 - \frac{\lambda \tau}{n} \right )^{n} \\
               & = \frac{n}{n} \left ( 1 - \frac{1}{n} \right ) \cdots \left ( 1 - \frac{k - 1}{n} \right ) \cdot \frac{1}{k!} \cdot (\lambda \tau)^{k}
                    \cdot \left ( 1 - \frac{\lambda \tau}{n} \right )^{-k} \cdot \left ( 1 - \frac{\lambda \tau}{n} \right )^{n}.
\end{align*}

Fix $k$ and let $n \to \infty$. In the limit we obtain:
\[
    P(k, \tau) = \frac{(\lambda \tau)^k}{k!} e^{- \lambda \tau}.
\]
Note that $P(0, \tau) = e^{- \lambda \tau}$ and $P(1, \tau) = \lambda \tau e^{- \lambda \tau}$. Recall that the Taylor series
for an infinitely differentiable function at $x = 0$ can be written as:
\[
    f(0 + \delta) = f(0) + f^{(1)}(0) \delta + \frac{f^{(2)}(0)}{2!} \delta^2 + \cdots + \frac{f^{(n)}(0)}{n!} \delta^n + \cdots
\]
The Taylor series expansion for $e^{-\lambda \tau}$ is:
\[
    e^{-(0 + \lambda \tau)} = 1 - \lambda \tau + \frac{(\lambda \tau)^2}{2!} - \frac{(\lambda \tau)^3}{3!} +
                        \cdots + (-1)^n \frac{(\lambda \tau)^n}{n!} + \cdots
\]
Using this, we obtain that $P(0, \tau) =  1 - \lambda \tau + o(\tau)$ and $P(1, \tau) = \lambda \tau + o_1(\tau)$, consistent with
the small-interval probability specifications.

\subsection{Mean and Variance}
The mean of a Poisson distribution can be easily calculated to be $\lambda \tau$. The variance of a Poisson distribution is also $\lambda \tau$.
This can also be seen from the limiting process under which we derived the Poisson. The mean of a Binomial is $np$ and the variance is $np - np^2$.
If we consider a situation in which $n \to \infty$, $p \to 0$ and $np$ is a constant, then the variance also tends to $np = \lambda \tau$.

\subsection{Time of First Arrival and Interarrival Times}
Let $Y_1$ denote the time of the first arrival. Then
\begin{align*}
    \Prone{Y_1 > t} & = \Prone{0 \text{ arrivals in } [0, t]} \\
                    & = e^{-\lambda t}.
\end{align*}
Therefore $1 - F_{Y_1}(t) =  e^{- \lambda t}$ and this yields that $f_{Y_1}(t) = \lambda e^{-\lambda t}$. Thus the time of the first
arrival is exponentially distributed. Now after the first arrival, one can think of the Poisson process ``restarting'' so that the
time till the next arrival is also exponentially distributed. This is so because of the memorylessness of the Poisson process.
Let $Y_k$ denote the time of the $k$th arrival and let $T_k$ be the time interval between the $(k - 1)$st and the $k$th arrival
(also called the $k$th inter-arrival time). Then
\begin{align*}
    Y_k & = T_1 + \cdots + T_k  \text{ for } k \in \{1, 2, \ldots\} \\
    T_k & = Y_k - Y_{k - 1} \text{ for } k \in \{2, 3, \ldots\} \\
    T_k & \iid \text{Exp}(\lambda) \text{ for } k \in \{1, 2, \ldots\}.
\end{align*}
What this means is that if the arrival process is assumed to be Poisson, then the inter-arrival times are exponentially distributed.
Intrestingly, the converse is also true.

Consider an arrival process with independent inter-arrival times $T_1, \ldots, T_k$ that are all exponentially distributed with
parameter $\lambda$. Fix a time interval $[0, \tau]$. Then the probability of $0$ arrivals in $[0, \tau]$ is the same as
$\Prone{T_1 > \tau}$. This is given by:
\begin{align*}
\Prone{T_1 > \tau} & = \int_{\tau}^{\infty} \lambda e^{- \lambda t} \dx t \\
        & = e^{- \lambda \tau}.
\end{align*}
But this exactly the expression for $0$ arrivals in a time interval $\tau$ in a Poisson process with rate $\lambda$. Next, consider
the case of exactly one arrival in the interval $[0, \tau]$. The event of exactly one arrival is the intersection
of the events that $T_1 = t$, where $0 \leq t \leq \tau$, and $T_2 > \tau - t$.
\begin{align*}
    \Prone{1 \text{ arrival in time } [0, \tau]} & = \int_{t = 0}^{\tau} \Prone{T_1 = t \text{ and } T_2 > \tau - t} \dx t \\
        & = \int_{t = 0}^{\tau} \lambda e^{- \lambda t} \left ( \int_{\tau - t}^{\infty} \lambda e^{-\lambda x } \dx x \right ) \dx t \\
        & = \int_{t = 0}^{\tau} \lambda e^{- \lambda t} e^{- \lambda (\tau - t)} \dx t \\
        & = \int_{t = 0}^{\tau} \lambda e^{- \lambda \tau} \\
        & = \lambda \tau e^{- \lambda \tau}.
\end{align*}
Again, this matches the expression for $1$ arrival in a Poisson process with rate $\lambda$ in an interval of length $\tau$. We will 
not show the full derivation here.

\subsection{Time of $k$th Arrival}
The time of the $k$th arrival~$Y_k$ follows a discrete Gamma or Erlang distribution. Let $f_{Y_k}$ be the pdf of the 
distribution. Imagine that the $k$th arrival happens in a time interval $[y, y + \delta]$. Now the $k$th arrival takes place 
in $[y, y + \delta]$ iff 
\begin{itemize}
    \item $A$: $k - 1$ arrivals happen in the time interval $[0, y]$, and 
    \item $B$: exactly one arrival happens in $[y, y + \delta]$. 
\end{itemize}
\begin{align*}
    \Prone{y \leq Y_k \leq y + \delta} & = \Prone{A} \cdot \Prone{B} \\
        & = \frac{(y \lambda)^{k - 1}}{(k - 1)!} e^{- \lambda y} \cdot y \delta \\
        & = \delta \cdot \frac{y^k \lambda^{k - 1}}{(k - 1)!} e^{- \lambda y}.  
\end{align*}  
Thus 
\[
    \Prone{Y_k = y} = \frac{y^k \lambda^{k - 1}}{(k - 1)!} e^{- \lambda y}.
\]
This is the discrete version of the Gamma distribution which is usually stated in terms of the parameters $\alpha$ and $\beta$, instead of $k$ and $\lambda$. 
\[
    \text{Gamma}(y \mid \alpha, \beta) =  \frac{\beta^{\alpha - 1}}{\Gamma(\alpha)} y^{\alpha} e^{- \beta y}.
\]
